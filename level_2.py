# -*- coding: utf-8 -*-
"""Level-2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1818aAZWypqXCzkLWIBZPREfC-KzAmF84
"""



import torch
import torchvision
import torchvision.transforms as transforms
from efficientnet_pytorch import EfficientNet
import torch.nn as nn
import torch.optim as optim
import copy
from torch.utils.data import DataLoader, random_split
import torch.nn.functional as F
from sklearn.metrics import accuracy_score

# Fine-tuned EfficientNetB0 Model (for Federated Learning)
class FineTunedEfficientNetB0Model(nn.Module):
    def __init__(self):
        super(FineTunedEfficientNetB0Model, self).__init__()
        self.model = EfficientNet.from_pretrained('efficientnet-b0')
        # Modify the final fully connected layer to match the MNIST dataset (10 classes)
        self.model._fc = nn.Linear(self.model._fc.in_features, 10)

    def forward(self, x):
        return self.model(x)

# Load MNIST dataset and partition it among clients
def load_data_and_partition(transform, n_clients=10):
    train_dataset = torchvision.datasets.MNIST(root="./data/mnist", train=True, download=True, transform=transform)
    test_dataset = torchvision.datasets.MNIST(root="./data/mnist", train=False, download=True, transform=transform)

    # Split the dataset into n_clients partitions
    split_size = len(train_dataset) // n_clients
    client_datasets = random_split(train_dataset, [split_size] * n_clients)

    client_loaders = [DataLoader(dataset, batch_size=32, shuffle=True) for dataset in client_datasets]
    test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)

    return client_loaders, test_loader

# Client Update function
def client_update(client_model, optimizer, train_loader, device, epochs=1):
    client_model.train()
    running_loss = 0.0
    correct = 0
    total = 0
    for epoch in range(epochs):
        for data, target in train_loader:
            data, target = data.to(device), target.to(device)
            optimizer.zero_grad()
            output = client_model(data)
            loss = F.cross_entropy(output, target)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()

            # Accuracy calculation
            _, predicted = torch.max(output, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()

    loss_avg = running_loss / len(train_loader)
    accuracy = 100 * correct / total
    return loss_avg, accuracy

# Aggregating Client Models on the Server
def server_aggregate(global_model, client_models):
    global_dict = global_model.state_dict()
    for key in global_dict.keys():
        # Convert client model state_dict values to float32 before stacking and averaging
        global_dict[key] = torch.stack([client_model.state_dict()[key].float() for client_model in client_models], 0).mean(0)
    global_model.load_state_dict(global_dict)

# Testing the model on test data
def test_model(model, test_loader, device):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            _, predicted = torch.max(output, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()

    accuracy = 100 * correct / total
    return accuracy

# Federated Learning loop
def federated_learning(n_clients=10, global_epochs=10, local_epochs=3):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    transform = transforms.Compose([
        transforms.Grayscale(num_output_channels=1),
        transforms.Resize((128, 128)),  # Resize to match EfficientNet input
        transforms.Lambda(lambda x: x.convert("RGB")),  # Convert grayscale to RGB
        transforms.ToTensor(),
        transforms.Normalize((0.5,), (0.5,))])

    client_loaders, test_loader = load_data_and_partition(transform, n_clients)

    global_model = FineTunedEfficientNetB0Model().to(device)
    client_models = [copy.deepcopy(global_model) for _ in range(n_clients)]

    optimizers = [optim.Adam(model.parameters(), lr=0.0001) for model in client_models]

    # Federated learning loop
    for epoch in range(global_epochs):
        print(f'Global Epoch {epoch+1}/{global_epochs}')

        # Local training for each client
        for client_idx in range(n_clients):
            print(f'Training Client {client_idx + 1}/{n_clients}')
            loss, accuracy = client_update(client_models[client_idx], optimizers[client_idx], client_loaders[client_idx], device, local_epochs)
            print(f'Client {client_idx + 1} - Loss: {loss:.4f}, Accuracy: {accuracy:.2f}%')

        # Aggregating models from all clients
        server_aggregate(global_model, client_models)

        # Test global model
        test_accuracy = test_model(global_model, test_loader, device)
        print(f'Global Model Test Accuracy after epoch {epoch + 1}: {test_accuracy:.2f}%')

    # Save the final global model
    torch.save(global_model.state_dict(), 'federated_efficientnet_b0_mnist.pth')
    print("Federated Learning process completed and model saved.")

if __name__ == '__main__':
    federated_learning(n_clients=10, global_epochs=10, local_epochs=3)

