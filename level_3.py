# -*- coding: utf-8 -*-
"""Level-3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zTylFMoQlEoM6FwexNi3oLfPXmLPDKoq
"""



import torch
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import random_split, DataLoader
import torch.nn as nn
import torch.optim as optim
import random
import copy
from efficientnet_pytorch import EfficientNet

# EfficientNetB0 Model (adapted for MNIST)
class EfficientNetB0Model(nn.Module):
    def __init__(self):
        super(EfficientNetB0Model, self).__init__()
        # Load EfficientNetB0 pre-trained model
        self.model = EfficientNet.from_pretrained('efficientnet-b0')

        # Modify the first convolutional layer to accept 1 channel (grayscale) instead of 3 (RGB)
        self.model._conv_stem = nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)

        # Modify the final fully connected layer for 10 classes (MNIST)
        self.model._fc = nn.Linear(self.model._fc.in_features, 10)

    def forward(self, x):
        return self.model(x)

# Client Update function (with malicious behavior option)
def client_update(client_model, optimizer, train_loader, device, epochs=1, is_malicious=False):
    client_model.train()
    for epoch in range(epochs):
        for data, labels in train_loader:
            data, labels = data.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = client_model(data)

            if is_malicious:
                # Malicious client performs targeted attack (label manipulation)
                wrong_labels = (labels + 1) % 10  # Shift labels by 1 for malicious attack
                loss = nn.CrossEntropyLoss()(outputs, wrong_labels)
            else:
                loss = nn.CrossEntropyLoss()(outputs, labels)

            loss.backward()
            optimizer.step()

# Robust Aggregation using Median and Trimmed Mean
def robust_aggregate(global_model, client_models, client_weights=None):
    global_dict = global_model.state_dict()

    for k in global_dict.keys():
        updates = torch.stack([client_models[i].state_dict()[k].float() for i in range(len(client_models))])

        # Calculate median of updates
        median_update = torch.median(updates, dim=0).values

        # Calculate deviation from median
        deviations = torch.abs(updates - median_update)

        # Sort deviations and create mask for non-extreme values
        sorted_devs, _ = torch.sort(deviations, dim=0)
        threshold = sorted_devs[len(client_models) // 2]  # Use median deviation as threshold

        # Create mask for updates within threshold
        mask = deviations <= threshold.unsqueeze(0)

        # Average non-outlier updates
        valid_updates = updates * mask.float()
        num_valid = torch.sum(mask.float(), dim=0)
        num_valid = torch.clamp(num_valid, min=1.0)  # Avoid division by zero

        aggregated = torch.sum(valid_updates, dim=0) / num_valid
        global_dict[k] = aggregated

    global_model.load_state_dict(global_dict)

# Test the global model
def test_model(model, test_loader, device):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for data, labels in test_loader:
            data, labels = data.to(device), labels.to(device)
            outputs = model(data)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    return 100 * correct / total

# Federated Learning process
def federated_learning(n_clients=10, global_epochs=10, local_epochs=2, num_malicious=2):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    # Load and partition data
    transform = transforms.Compose([transforms.Resize(32),  # Resize to 32x32
                                    transforms.ToTensor(),
                                    transforms.Normalize((0.5,), (0.5,))])
    train_dataset = torchvision.datasets.MNIST(root="./data", train=True, download=True, transform=transform)
    test_dataset = torchvision.datasets.MNIST(root="./data", train=False, download=True, transform=transform)

    # Split training data among clients
    split_size = len(train_dataset) // n_clients
    client_datasets = random_split(train_dataset, [split_size] * n_clients)
    client_loaders = [DataLoader(dataset, batch_size=64, shuffle=True) for dataset in client_datasets]
    test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)

    # Initialize models
    global_model = EfficientNetB0Model().to(device)
    client_models = [copy.deepcopy(global_model) for _ in range(n_clients)]
    optimizers = [optim.Adam(model.parameters(), lr=0.001) for model in client_models]

    # Select malicious clients
    malicious_clients = set()
    while len(malicious_clients) < num_malicious:
        malicious_clients.add(random.randrange(n_clients))
    print(f"Malicious clients: {malicious_clients}")

    best_accuracy = 0
    for epoch in range(global_epochs):
        print(f"\nGlobal Epoch {epoch + 1}/{global_epochs}")

        # Client updates
        for i in range(n_clients):
            is_malicious = i in malicious_clients
            client_update(client_models[i], optimizers[i], client_loaders[i], device, local_epochs, is_malicious)

        # Robust aggregation
        robust_aggregate(global_model, client_models)

        # Test global model
        accuracy = test_model(global_model, test_loader, device)
        print(f"Global Model Test Accuracy: {accuracy:.2f}%")

        if accuracy > best_accuracy:
            best_accuracy = accuracy
            torch.save(global_model.state_dict(), 'best_robust_model.pth')

        # Update client models with global model
        for client_model in client_models:
            client_model.load_state_dict(global_model.state_dict())

    print(f"\nBest accuracy achieved: {best_accuracy:.2f}%")

if __name__ == "__main__":
    federated_learning(n_clients=10, global_epochs=10, local_epochs=2, num_malicious=2)

